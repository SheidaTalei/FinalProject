{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SheidaTalei_LSTM_FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SheidaTalei/FinalProject/blob/main/SheidaTalei_LSTM_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aphu-A5aCfZ8",
        "outputId": "991e4bfc-7246-4933-bbb7-ee71e9315601"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHzi9pdAnX1r",
        "outputId": "be953211-62c9-4aba-f1fe-d647d30dc405"
      },
      "source": [
        "!pip install --target=$nb_path hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\r\u001b[K     |█                               | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 27.5MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 21.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 24.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51kB 24.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61kB 27.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 122kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 153kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 184kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 194kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 215kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 225kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 245kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 256kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 307kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 18.2MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 56.9MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp37-none-any.whl size=1394470 sha256=d4db6969b2a015fe0ad2615bce424099857ce769aeffa4b1cb38531abbac8c6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=155039 sha256=65a3bcd3b88cc01c5b932e5764a664eefdc5ed04a750c8e9f45e8748840c3c62\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nrjfl8EnXyU",
        "outputId": "40ceec71-4941-49dd-af10-2380afa7e4df"
      },
      "source": [
        "!pip install --target=$nb_path sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_GdEDnpnXv8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "het5g_T6nXts"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yv90hltCOCz"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import regex\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzzSJswNBZMU"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Conv1D\n",
        "\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.metrics import  auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op2BCxa7DQo_"
      },
      "source": [
        "#------------------------------------------Empty Rows Removal----------------------------------------------------------------\n",
        "# This function Will: 1- Remove all empty rows from csv file 2- Save data to the same csv\n",
        "def removeEmptyRows(fileName):\n",
        "    df = pd.read_csv(fileName, encoding='utf-8-sig')\n",
        "    df = df.dropna(subset=['text'], how='all', axis=0) \n",
        "    df.to_csv(fileName, header=True, encoding='utf-8-sig',  index=False)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvHKT874KjUQ"
      },
      "source": [
        "# df1= removeEmptyRows ('/content/drive/MyDrive/Final/Prepared_Data_1_Labeled.csv')\r\n",
        "# df2 =  removeEmptyRows ('/content/drive/MyDrive/Final/Prepared_train_Data_test#1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxtbupMfSwTX"
      },
      "source": [
        "# X_and_Y =pd.concat([df1, df2], ignore_index=True, names=['tweeter_handle', 'text', 'Label', 'statues_count' ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9riVgKfRCpZ9"
      },
      "source": [
        "# X_and_Y = removeEmptyRows ('/content/drive/MyDrive/Final/Temp.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPWDrM3PSjhY"
      },
      "source": [
        "# train_data, test_data = train_test_split(X_and_Y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbkTBwewpruI"
      },
      "source": [
        "X_and_Y =removeEmptyRows('/content/drive/MyDrive/Final/train_temp.csv')\n",
        "Y= X_and_Y.Label\n",
        "X = X_and_Y.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlCKP5_5prmZ"
      },
      "source": [
        "X_and_Y_test = removeEmptyRows('/content/drive/MyDrive/Final/test_temp.csv') #Prepared_test_Data_00 \n",
        "Y_test= X_and_Y_test.Label\n",
        "X_test = X_and_Y_test.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHs3klBg10FN"
      },
      "source": [
        "#This part helps for handeling imbalanced data\n",
        "count_0 = 0\n",
        "count_1 = 0\n",
        "\n",
        "for i in Y:\n",
        "    if (i==0):\n",
        "        count_0 = count_0+1\n",
        "    else:\n",
        "        count_1 = count_1 + 1\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1SDb_x0qGUQ"
      },
      "source": [
        "p = count_0 / (count_0 + count_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI8yA4fSozq5"
      },
      "source": [
        "print (\"count_0: \",count_0 )\n",
        "print (\"count_1: \",count_1 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwL9hZ08fSbe"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEhEElBKBZNC"
      },
      "source": [
        "#converting words to vectors\n",
        "max_len = 150 #120\n",
        "tokenizer = Tokenizer(num_words = max_len, lower = True, split = ' ')\n",
        "tokenizer.fit_on_texts(X )\n",
        "\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "#pad sequence is used to ensure that all sequences in a list have the same length.\n",
        "X_ready = pad_sequences(sequences = X, maxlen = max_len, padding = 'pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c4XDKwhBZNH"
      },
      "source": [
        "\n",
        "def createLstmModel(units = 200, dropout = 0.5  ):\n",
        "    \n",
        "    \n",
        "   #https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
        "        \n",
        "    #Adding Layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(5000 , 128 ))#max_feature = 5000 , embedding_dimention = 128\n",
        "\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))  \n",
        "   \n",
        "    # model.add(LSTM(units = units , dropout = 0.2, return_sequences=True ))\n",
        "    # model.add(Dense(units = units, activation=\"relu\"))\n",
        "    # model.add(Dropout(rate = dropout))\n",
        "\n",
        "\n",
        "  \n",
        "    model.add(LSTM(units = units , dropout = 0.2))\n",
        "    model.add(Dense(units = units, activation=\"relu\"))\n",
        "    model.add(Dropout(rate = dropout))\n",
        "\n",
        "    model.add(Dense(units = 1 ,activation=\"sigmoid\"))\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[f1_m ,tf.keras.metrics.AUC(), 'accuracy'])\n",
        "    \n",
        "    # model.fit(X, Y, batch_size = 150, class_weight = {0: 1/66639, 1: 1/(1- (1/66639))})\n",
        "#     model.summary()\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0Slag_sPaFK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzMWk1lx21Mh"
      },
      "source": [
        "#source: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_load.ipynb#scrollTo=IFPuhwntH8VH\n",
        "import os\n",
        "checkpoint_path = \"/content/drive/MyDrive/Final/lstm1.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK2B8Uf7STHe"
      },
      "source": [
        "# print(type(X_ready))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFg3w9kRBZNN"
      },
      "source": [
        "# Fit LSTM model\n",
        "#source: https://apmonitor.com/do/index.php/Main/LSTMNetwork\n",
        "\n",
        "# history = model.fit(X_ready, Y , epochs = 2, batch_size = 150, class_weight = {0: 1/66639, 1: 1/(1-66639)})\n",
        "\n",
        "#400\n",
        "model = createLstmModel()\n",
        "history = model.fit(X_ready, \n",
        "          Y,  \n",
        "          epochs=200,\n",
        "          batch_size = 150,\n",
        "          class_weight = {0: 1/p, 1: 1/(1-p)} , validation_split = 0.3,callbacks=[cp_callback]) \n",
        "# plt.figure()\n",
        "# plt.ylabel('loss'); plt.xlabel('epoch')\n",
        "# plt.semilogy(history.history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h0cYcXhViIl"
      },
      "source": [
        "#source: https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRtUqj__VWEN"
      },
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "plt.subplot(1,2,1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None,1)\n",
        "plt.subplot(1,2,2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0,None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1SvJZGg58qP"
      },
      "source": [
        "import os\n",
        "checkpoint_path = \"/content/drive/MyDrive/Final/lstm1.ckpt\"\n",
        "model = createLstmModel()\n",
        "model.load_weights(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_VhYnq4BZNQ"
      },
      "source": [
        "# class_weight = {0: 1/66639, 1: 1/(1- (1/66639))}\n",
        "# model = KerasClassifier(build_fn=createLstmModel , epochs=1000   )\n",
        "\n",
        "# # class_weights = class_weight.compute_class_weight('balanced', np.unique(Y), Y)\n",
        "# # pipeline =  [\n",
        "# # #         ('tokenizer', Tokenizer(num_words = max_feature, lower = True, split = ' ')),\n",
        "# # #         ('pad', pad_sequences(sequences = X , maxlen = max_feature, padding = 'pre')),\n",
        "# #         ('estimator', model)\n",
        "# #     ]\n",
        "\n",
        "# parameters = { 'dropout' : [0.5,0.1, 0.01], 'units':[100,  300, 700, 1000] , 'batch_size' :[150]\n",
        "# #     'tokenizer__num_words': (100, 200, 300),\n",
        "# #     'pad__maxlen':( 200, 300, 400),\n",
        "    \n",
        "#     # 'estimator__dropout': (0.5,0.1, 0.01),\n",
        "#     # 'estimator__units' :(100,  300, 700, 1000)\n",
        "   \n",
        "# }\n",
        "\n",
        "\n",
        "# #source: https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
        "\n",
        "# # grid_search_tune = GridSearchCV(Pipeline(pipeline), parameters, cv=20, verbose=1  )\n",
        "\n",
        "# grid_search_tune = GridSearchCV(model , parameters, cv=20 , verbose=1)\n",
        "\n",
        "# LSTM = grid_search_tune.fit(X, Y)\n",
        "#?????????? check batch_size and Class_weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6kfl2FVBZNU"
      },
      "source": [
        "# print (grid_search_tune.best_estimator_.steps)\n",
        "\n",
        "# print ('Best Parameter for LSTM ',grid_search_tune.best_params_)\n",
        "# print('Best Score for LSTM: ',grid_search_tune.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "httu2ESUBZNX"
      },
      "source": [
        "\n",
        "# tokenizer = Tokenizer(num_words = max_len, lower = True, split = ' ')\n",
        "#converting words to vectors\n",
        "tokenizer.fit_on_texts(X_test )\n",
        "\n",
        "X_temp = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "#pad sequence is used to ensure that all sequences in a list have the same length.\n",
        "X_ready_test = pad_sequences(sequences = X_temp, maxlen = max_len, padding = 'pre')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hnhjannBZNa"
      },
      "source": [
        "y_predict =model.predict(X_ready_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_UAA8qzKwcm"
      },
      "source": [
        "# print(y_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55u_D1VsBZNd"
      },
      "source": [
        "# print('Accuracy: ' , accuracy_score( Y_test,y_predict))\n",
        "# #Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
        "# print('F1-score: ', f1_score(Y_test, y_predict))\n",
        "# #source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\n",
        "# print('roc_auc_score: ', roc_auc_score(Y_test, y_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phGDKAmbvAw4"
      },
      "source": [
        "loss, f1_score, auc,accuracy = model.evaluate(X_ready_test, Y_test, verbose=0)\n",
        "print('Accuracy:  ' , accuracy)\n",
        "\n",
        "print('F1-score: ', f1_score)\n",
        "\n",
        "print('roc_auc_score: ', auc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHrwAEXb-nvo"
      },
      "source": [
        "# train_data.to_csv('/content/drive/MyDrive/Final/train_temp.csv' ,index=False, encoding='utf-8-sig')\r\n",
        "# test_data.to_csv('/content/drive/MyDrive/Final/test_temp.csv' ,index=False, encoding='utf-8-sig')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxaeh7l3V2O8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}