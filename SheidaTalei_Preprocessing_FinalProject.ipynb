{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SheidaTalei_Preprocessing_FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SheidaTalei/FinalProject/blob/main/SheidaTalei_Preprocessing_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBMC55aoFpyY"
      },
      "source": [
        "# SUBJECT: Preprocessing \n",
        "## Author:  Sheida Talei"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpbrmpuLkuG4",
        "outputId": "61c00a97-d931-4fa3-86f6-e8b5264ca417"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCEht8VNldKu",
        "outputId": "8fc3ee95-f21c-496c-acf5-bcc3e2b166c1"
      },
      "source": [
        "!pip install --target=$nb_path hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "  Using cached https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab/libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c/nltk-3.3-cp37-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n",
            "Installing collected packages: libwapiti, nltk, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkAo7I6TlyI6",
        "outputId": "862ff09c-1681-4dfc-9f57-12f232088060"
      },
      "source": [
        "!pip install --target=$nb_path persianutils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting persianutils\n",
            "  Using cached https://files.pythonhosted.org/packages/80/9c/5c382b9f45afc205e6c26cc94ed55b17949f3fa3a827f53571a6796257a4/persianutils-0.1.2-py3-none-any.whl\n",
            "Installing collected packages: persianutils\n",
            "Successfully installed persianutils-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKTF3dLMa1rl",
        "outputId": "51c97217-b29b-4d40-ae78-2789b207354f"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bki6EZptFpyZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import regex\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from hazm import *\n",
        "import re\n",
        "import string\n",
        "import persianutils as pu\n",
        "import persianutils.PersianAlphabet as pALpha\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUAJ16rVFpyZ"
      },
      "source": [
        "#------------------------------------------Empty Rows Removal----------------------------------------------------------------\n",
        "# This function Will: 1- Remove all empty rows from csv file 2- Save data to the same csv\n",
        "def removeEmptyRows(fileName):\n",
        "    df = pd.read_csv(fileName, encoding='utf-8-sig')\n",
        "    df = df.dropna(subset=['text'], how='all', axis=0) \n",
        "    df.to_csv(fileName, header=True, encoding='utf-8-sig',  index=False)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ7tkRw3FpyZ"
      },
      "source": [
        "#------------------------------------------------Normalization---------------------------------------------------------------\n",
        "# This function will: 1- Normalize tweets (replace space with half-space)\n",
        "def normalization (content):\n",
        "    normalizer = Normalizer()\n",
        "    tweets_normalize_list = []\n",
        "    temp_set = []\n",
        "    for item in range (len(content)):\n",
        "        for word in content[item]:\n",
        "            temp_word = normalizer.normalize(word)\n",
        "            temp_set.append(temp_word.replace('\\u200c', ' ', 1000000000))\n",
        "                \n",
        "                \n",
        "        tweets_normalize_list.append(set(temp_set))\n",
        "        temp_set = []\n",
        "        \n",
        "    return tweets_normalize_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYUV-xNOFpya"
      },
      "source": [
        "#---------------------------------------------------- Latin and Punctuation Removal-------------------------------------------\n",
        "# This function will: 1- Remove latin words 2- Remove punctuations\n",
        "# Source: https://stackoverflow.com/questions/18429143/strip-punctuation-with-regex-python/50985687\n",
        "def latinRemoval(content):\n",
        "    only_persian_tweet_list =[]\n",
        "    temp_set = []\n",
        "\n",
        "    latin = re.compile(r'[a-zA-Z0-9@$!%*?&-_`.+#(»)«,:;،؛…..؟]+')  \n",
        "\n",
        "    for item in range(len(content)):\n",
        "        for word in content[item]:\n",
        "            temp_word = word.replace('\\u200c', ' ', 1000000000)\n",
        "               \n",
        "            if not(latin.match(temp_word)) :\n",
        "                if not temp_word.isdigit():\n",
        "                        #source: https://pypi.org/project/persianutils/\n",
        "                    word = re.findall(r'[^۰-۹]+', temp_word.strip(string.punctuation))\n",
        "                    \n",
        "                    if len(word) >0:    \n",
        "                        processed_text = pu.standardize4Word2vec(word[0].strip())\n",
        "\n",
        "                        if (processed_text != ' '):\n",
        "#                             print(processed_text)\n",
        "                            word = re.findall('\\d+', processed_text.strip())\n",
        "                            if len(word)==0 : \n",
        "                                temp_set.append(processed_text.strip())\n",
        "                                # print (processed_text.strip())\n",
        "           \n",
        "        only_persian_tweet_list.append(set(temp_set))\n",
        "        temp_set = []\n",
        "        \n",
        "    return only_persian_tweet_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjXUt_VLFpya"
      },
      "source": [
        "#--------------------------------------------------Loading StopWords ------------------------------------------\n",
        "#Source of file: https://sites.google.com/site/kevinbouge/stopwords-lists\n",
        "def getStopWord ():\n",
        "    try:\n",
        "        file = open('/content/drive/MyDrive/Final/stopwords_fa.txt', 'r', encoding='utf-8-sig')\n",
        "        file_readed = file.read()\n",
        "    \n",
        "    finally:\n",
        "        file.close()\n",
        "        \n",
        "    stopWord_Set = set(file_readed.split())\n",
        "    return stopWord_Set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krHkmqYQFpya"
      },
      "source": [
        "#------------------------------------Tokenization and Stop Word Removal------------------------------------------------------\n",
        "# This function will: 1- Tokenize words  2- Remove stop words and return a list of sets\n",
        "def getTweetNoStopWord(content):\n",
        "    tweet_no_StopWord_list =[]\n",
        "    temp_set = []\n",
        "    stop_word_list = getStopWord()\n",
        "    for item in range(len(content)):\n",
        "        for word in content[item]:\n",
        "            if not word in stop_word_list:\n",
        "                temp_set.append(word.replace('\\u200c', ' ', 1000000000))\n",
        "                \n",
        "        tweet_no_StopWord_list.append(set (temp_set))\n",
        "        temp_set = []\n",
        "        \n",
        "    return tweet_no_StopWord_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIv3abWoFpya"
      },
      "source": [
        "#----------------------------------------------Lemmatization ------------------------------------------------------------------\n",
        "def Lemmatization (content):\n",
        "    lemmatizer = Lemmatizer()\n",
        "    tweets_lemmatize_list = []\n",
        "    temp_set = []\n",
        "    for item in range(len(content)):\n",
        "        for word in (content[item]):\n",
        "            temp_set.append(lemmatizer.lemmatize(word.replace('\\u200c', ' ', 1000000000)))\n",
        "        tweets_lemmatize_list.append(set(temp_set))\n",
        "        temp_set = []\n",
        "    \n",
        "    return tweets_lemmatize_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruS6NDw9Fpya"
      },
      "source": [
        "#---------------------------------------------------Tokenization-------------------------------------------------------------\n",
        "# Tokenizing X and at the end append Y as a label\n",
        "def tokenization (X):\n",
        "    tokenized_train_data = []\n",
        "    for item in range(len(X)):\n",
        "        var = word_tokenize(X[item])\n",
        "#         var.append(Y[item])\n",
        "        tokenized_train_data.append(var)\n",
        "        \n",
        "    return tokenized_train_data    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5qOdJgpFpya"
      },
      "source": [
        "#Loading data\n",
        "\n",
        "# dfPrime_1 = pd.read_csv ('/content/drive/MyDrive/Final/Prime#1.csv', encoding='utf-8-sig')\n",
        "# dfPrime_2 = pd.read_csv ('/content/drive/MyDrive/Final/Prime#2.csv', encoding='utf-8-sig')\n",
        "# dfPrime_3 = pd.read_csv ('/content/drive/MyDrive/Final/Prime#3.csv', encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "# data_frame =  pd.read_csv ('/content/drive/MyDrive/Final/Prepared_Data_1_Labeled.csv', encoding='utf-8-sig')\n",
        "# df2 =  pd.read_csv ('/content/drive/MyDrive/Final/Prepared_test_Data_test#1.csv', encoding='utf-8-sig')\n",
        "\n",
        "# df2 = pd.read_csv ('/content/drive/MyDrive/Final/Main#33.csv', encoding='utf-8-sig')\n",
        "# df3 = pd.read_csv ('/content/drive/MyDrive/Final/Main#34.csv', encoding='utf-8-sig')\n",
        "# df4 = pd.read_csv ('/content/drive/MyDrive/Final/Main#35.csv', encoding='utf-8-sig')\n",
        "# df5 = pd.read_csv ('/content/drive/MyDrive/Final/Main#36.csv', encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "df1= removeEmptyRows ('/content/drive/MyDrive/Final/train_temp.csv')\n",
        "df2 =  removeEmptyRows ('/content/drive/MyDrive/Final/test_temp.csv')\n",
        "# data_frame = removeEmptyRows('/content/drive/MyDrive/Final/test_temp.csv')\n",
        "\n",
        "data_frame= pd.concat([df1, df2 ], ignore_index=True, names=['tweeter_handle', 'text', 'Label', 'statues_count' ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJlgUPqzFpya",
        "scrolled": false
      },
      "source": [
        "#----------------------------------------------------------Step1----------------------------------------------------------\n",
        "#----------------------------------Normalization_StopWord_Lemmatization_Punctuaation--------------------------------------\n",
        "#Reading just coulmns of description and Label\n",
        "# Remove repetetive tweets\n",
        "def cleanData (df):\n",
        "    df.drop_duplicates(subset=['text', 'tweeter_handle'], inplace=True)\n",
        "    # train_data, test_data = train_test_split(df, test_size=0.2)\n",
        "    # print('train data size: {}    test data size: {}'.format(len(train_data), len(test_data)))\n",
        "    header = ['tweeter_handle', 'text', 'Label', 'statues_count']\n",
        "    df.to_csv('/content/drive/MyDrive/Final/X_and_Y_test#1.csv', index=False, encoding='utf-8-sig' , columns = header )\n",
        "    \n",
        "\n",
        "\n",
        "    #Step 1: Removing empty rows from data set which contains only description and Label\n",
        "    # This functon will remove empty rows and save them to same file (input file)\n",
        "    removeEmptyRows('/content/drive/MyDrive/Final/X_and_Y_test#1.csv')\n",
        "    \n",
        "    #test\n",
        "    # test_data.to_csv('/content/drive/MyDrive/Final/X_and_Y_test.csv', index=False, encoding='utf-8-sig')\n",
        "    # #Step 1: Removing empty rows from data set which contains only description and Label\n",
        "    # # This functon will remove empty rows and save them to same file (input file)\n",
        "    # removeEmptyRows('/content/drive/MyDrive/Final/X_and_Y_test.csv')\n",
        "    \n",
        "    return df\n",
        "\n",
        "#---------------------------------------------test-------------------------------------------------------------------------\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgVFBl5zFpya"
      },
      "source": [
        "cleanData (data_frame)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwBOh7P2Fpyc"
      },
      "source": [
        "#------------------------------------------------------Step 2------------------------------------------------------------------\n",
        "#step 2: Tokenizing data\n",
        "X_and_Y = pd.read_csv('/content/drive/MyDrive/Final/X_and_Y_test#1.csv',encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "\n",
        "X = X_and_Y.text\n",
        "Y= X_and_Y.Label\n",
        "\n",
        "\n",
        "#Tokenization\n",
        "tokenize_list = tokenization (X)\n",
        "#------------------------------------------------------test---------------------------------------------------------------------\n",
        "#step 2: Tokenizing data\n",
        "# X_and_Y_test = pd.read_csv('/content/drive/MyDrive/Final/X_and_Y_test.csv',encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "\n",
        "# X_test = X_and_Y_test.text\n",
        "# Y_test= X_and_Y_test.Label\n",
        "\n",
        "# # #Tokenization\n",
        "# train_data_tokenize_list_test = tokenization (X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQQhsdkwFpyc"
      },
      "source": [
        "print(tokenize_list[3755])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZSQHgzmFpyc"
      },
      "source": [
        "#------------------------------------------------------Step 3------------------------------------------------------------------\n",
        "#step 3: removing HASHTAGS, MENTIONS, NUMBERS and any LATIN WORD\n",
        "# This function get tokenized list of data and remove mentioned elements\n",
        "\n",
        "only_persian_tweet_list = latinRemoval(tokenize_list)\n",
        "#------------------------------------------------------test--------------------------------------------------------------------\n",
        "#step 3: removing HASHTAGS, MENTIONS, NUMBERS and any LATIN WORD\n",
        "# This function get tokenized list of data and remove mentioned elements\n",
        "\n",
        "# only_persian_tweet_list_test = latinRemoval(train_data_tokenize_list_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_hUzuZOFp0F"
      },
      "source": [
        "print(tokenize_list[3755])\n",
        "print ('  ')\n",
        "print(only_persian_tweet_list[3755])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyuHSaqEFp0F"
      },
      "source": [
        "#------------------------------------------------------Step 4------------------------------------------------------------------\n",
        "#step 4: Normalization\n",
        "# normalize_tweet_list = normalization (only_persian_tweet_list)\n",
        "\n",
        "#-------------------------------------------------------test-------------------------------------------------------------------\n",
        "#step 4: Normalization\n",
        "# normalize_tweet_list_test = normalization (only_persian_tweet_list_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYIMTLF5Fp0F"
      },
      "source": [
        "# print(normalize_tweet_list[3755])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mmeUb8PFp0G"
      },
      "source": [
        "#------------------------------------------------------Step 5------------------------------------------------------------------\n",
        "#step 5: Remove stop words\n",
        "noStopWord_tweet_List = getTweetNoStopWord(only_persian_tweet_list)\n",
        "\n",
        "#-------------------------------------------------------test-------------------------------------------------------------------\n",
        "#step 5: Remove stop words\n",
        "# noStopWord_tweet_List_test = getTweetNoStopWord(normalize_tweet_list_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8krQGxdcFp0G"
      },
      "source": [
        "print(noStopWord_tweet_List[3755])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARLmIjRTFp0G"
      },
      "source": [
        "#------------------------------------------------------Step 6------------------------------------------------------------------\n",
        "#step 6: Lemmatixation\n",
        "lemmatize_tweet_list = Lemmatization (noStopWord_tweet_List)\n",
        "#-------------------------------------------------------test-------------------------------------------------------------------\n",
        "#step 6: Lemmatixation\n",
        "# lemmatize_tweet_list_test = Lemmatization (noStopWord_tweet_List_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqX7bwpcFp0G"
      },
      "source": [
        "#Source : https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/\n",
        "#shows the proportion of the whole data both train and test\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.countplot(x='Label', data=cleanData (data_frame))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJGWRBsiFp0H"
      },
      "source": [
        "def convertSequenceWordsToText(input_list,  fileName, lastFileName):\n",
        "    X_and_Y = pd.read_csv(fileName , encoding=\"utf-8\")\n",
        "    text = ''\n",
        "    \n",
        "    counter = 0\n",
        "    updator = 0\n",
        "    for item in range(len(input_list)):\n",
        "      if counter <= 80000:\n",
        "        for i in range(len(input_list [item + updator])):\n",
        "          text += list(input_list[item + updator])[i] + ' '\n",
        "      else:\n",
        "        break\n",
        "      \n",
        "      print(counter)    \n",
        "      X_and_Y.loc [counter, 'text'] = text.strip()\n",
        "      X_and_Y.to_csv(lastFileName ,index=False, encoding='utf-8-sig')\n",
        "      counter = counter + 1\n",
        "      text =''\n",
        "    print(\"This is counter: \" ,counter-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFdDLUDTKXGO"
      },
      "source": [
        "# load additional module\n",
        "import pickle\n",
        "\n",
        "# define a list of places\n",
        "\n",
        "file_name = \"/content/drive/MyDrive/Final/lemmatize_tweet_list_test#1.pkl\"\n",
        "open_file = open(file_name, \"wb\")\n",
        "pickle.dump(lemmatize_tweet_list, open_file)\n",
        "open_file.close()\n",
        "\n",
        "file_name = \"/content/drive/MyDrive/Final/lemmatize_tweet_list_test#1.pkl\"\n",
        "open_file = open(file_name, \"rb\")\n",
        "lemmatize_tweet_list = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8QuAjgBb0oY",
        "outputId": "cef3bd54-f9fe-472f-ac22-aad37af04f13"
      },
      "source": [
        "print(lemmatize_tweet_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'خدا', 'حالا', 'ایران', 'معطل', 'کنه', 'جمهوریخواه', 'مانده', 'حرومزاده', 'لعنت', 'انداخت#انداز', 'انقلاب', 'کردن', 'دموکرات', 'الاغ', 'فیل', 'تحریم'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Tkod6vFp0H"
      },
      "source": [
        "convertSequenceWordsToText(lemmatize_tweet_list, '/content/drive/MyDrive/Final/X_and_Y_test#1.csv', '/content/drive/MyDrive/Final/Prepared_Data_test#1.csv' )\n",
        "# convertSequenceWordsToText(lemmatize_tweet_list,'/content/drive/MyDrive/Final/Prepared_Data_test#1.csv', '/content/drive/MyDrive/Final/Prepared_Data_test#1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZmmrT4gFp0H"
      },
      "source": [
        "\n",
        "# convertSequenceWordsToText(lemmatize_tweet_list_test, '/content/drive/MyDrive/Final/X_and_Y_test.csv', '/content/drive/MyDrive/Final/Prepared_test_Data.csv' )\n",
        "# convertSequenceWordsToText(lemmatize_tweet_list_test,'/content/drive/MyDrive/Final/Prepared_test_Data.csv', '/content/drive/MyDrive/Final/Prepared_test_Data.csv' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDghNDmXxWUM"
      },
      "source": [
        "#divide data to train and test\n",
        "dataFrame =  pd.read_csv ('/content/drive/MyDrive/Final/Prepared_Data_test#1.csv', encoding='utf-8-sig')\n",
        "train_data, test_data = train_test_split(dataFrame, test_size=0.2)\n",
        "print('train data size: {}    test data size: {}'.format(len(train_data), len(test_data)))\n",
        "train_data.to_csv('/content/drive/MyDrive/Final/Prepared_train_Data_test#1.csv' ,index=False, encoding='utf-8-sig')\n",
        "test_data.to_csv('/content/drive/MyDrive/Final/Prepared_test_Data_test#1.csv' ,index=False, encoding='utf-8-sig')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm_7KVeNLS4-"
      },
      "source": [
        "count_0 = 0\n",
        "count_1 = 0\n",
        "Y = test_data.Label\n",
        "for i in Y:\n",
        "    if (i==0):\n",
        "        count_0 = count_0+1\n",
        "    else:\n",
        "        count_1 = count_1 + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcELli9xLqxM",
        "outputId": "0fb6ec2b-aa16-437a-e716-2856cc625d87"
      },
      "source": [
        "print(\"count_0: \",count_0)\n",
        "print(\"count_1: \",count_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count_0:  29044\n",
            "count_1:  5223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbP0-jaeLxSq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}