{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SheidaTalei_KMeanVsTfIDf_FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOP5rnjm8A6tSuCwxNl/l3N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SheidaTalei/FinalProject/blob/main/SheidaTalei_KMeanVsTfIDf_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5kbpylkbSHh"
      },
      "source": [
        "#SUBJECT: K-Mean Vs TF-IDF\r\n",
        "###AUTHOR: Sheida Talei"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uovInzwGbQEC",
        "outputId": "3ba621a9-81ef-4a02-e423-f4f1b05578f5"
      },
      "source": [
        "import os, sys\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "nb_path = '/content/notebooks'\r\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\r\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-P86Dhv3-X5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad474912-2119-4c73-c36c-b19ee23c003b"
      },
      "source": [
        "!pip install --target=$nb_path hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6eSIHMoj-xR",
        "outputId": "985a5e83-6cf0-40a8-f6da-2275b2e8245e"
      },
      "source": [
        "pip install -U sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "538pDnGPbP5F"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns; sns.set()  # for plot styling\r\n",
        "from kneed import KneeLocator\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5txGvMpNIdl"
      },
      "source": [
        "from os import path\r\n",
        "from PIL import Image\r\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sn\r\n",
        "from wordcloud_fa import WordCloudFa\r\n",
        "# from persian_wordcloud.wordcloud import PersianWordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hOtYeMRmX3S"
      },
      "source": [
        "from hazm import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYAZ4WdxmXzz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvP1wTWAmXxk"
      },
      "source": [
        "def tokenization (X):\r\n",
        "    tokenized_train_data = []\r\n",
        "    for item in range(len(X)):\r\n",
        "        var = word_tokenize(X[item])\r\n",
        "#         var.append(Y[item])\r\n",
        "        tokenized_train_data.append(var)\r\n",
        "        \r\n",
        "    return tokenized_train_data    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MMBLXDdb2U8"
      },
      "source": [
        "#------------------------------------------Empty Rows Removal----------------------------------------------------------------\r\n",
        "# This function Will: 1- Remove all empty rows from csv file 2- Save data to the same csv\r\n",
        "def removeEmptyRows(fileName):\r\n",
        "    df = pd.read_csv(fileName, encoding='utf-8-sig')\r\n",
        "    df = df.dropna(subset=['text'], how='all', axis=0) \r\n",
        "    df.to_csv(fileName, header=True, encoding='utf-8-sig',  index=False)\r\n",
        "    \r\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V2U_CB1b2Ol"
      },
      "source": [
        "\r\n",
        "X_and_Y = removeEmptyRows ('/content/drive/MyDrive/Final/train_temp.csv')\r\n",
        "# X_and_Y = removeEmptyRows ('/content/drive/MyDrive/Final/WightedTweets.csv')\r\n",
        "Y= X_and_Y.Label\r\n",
        "X = X_and_Y.text\r\n",
        "# X = X_and_Y.tweet\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvC1oIP724dZ"
      },
      "source": [
        "Y = X_and_Y.Weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUWlk3Zvm_8Y"
      },
      "source": [
        "\r\n",
        "list_x_and_y = tokenization (X) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3q_ry2Covq7"
      },
      "source": [
        "print(list_x_and_y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7QL5eNPcd1j"
      },
      "source": [
        "#--------------------------------------------------Loading StopWords ------------------------------------------\r\n",
        "#Source of file: https://sites.google.com/site/kevinbouge/stopwords-lists\r\n",
        "def getStopWord ():\r\n",
        "    try:\r\n",
        "        file = open('/content/drive/MyDrive/Final/stopwords_fa.txt', 'r', encoding='utf-8-sig')\r\n",
        "        file_readed = file.read()\r\n",
        "    \r\n",
        "    finally:\r\n",
        "        file.close()\r\n",
        "        \r\n",
        "    stopWord_Set = set(file_readed.split())\r\n",
        "    return stopWord_Set\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqzSeEf_coFT"
      },
      "source": [
        "persian_stop_word = list(getStopWord())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BLb9YzVd491"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words= persian_stop_word ,  max_features = 2000 , ngram_range=(1,4) ) \r\n",
        "X_train_vector  = vectorizer.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjPYeZcpp562"
      },
      "source": [
        "#source: https://stackoverflow.com/questions/34725726/is-it-possible-apply-pca-on-any-text-classification\r\n",
        "svd = TruncatedSVD(n_components=200)\r\n",
        "X_svd = svd.fit_transform(X_train_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW9xskck8nWK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcXSoDF5cn8j"
      },
      "source": [
        "#source: https://towardsdatascience.com/k-means-clustering-with-python-code-explained-5a792bd19548\r\n",
        "# num_cluster = 1000\r\n",
        "# kmean=KMeans(n_clusters=num_cluster)\r\n",
        "# kmean.fit(X_svd)\r\n",
        "# labels = kmean.labels_\r\n",
        "# silhoute = metrics.silhouette_score(X_svd, labels, metric='euclidean')\r\n",
        "# print(\"silhouette_score is: \",silhoute)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZbUEqAlLICm"
      },
      "source": [
        "# centroids = kmean.cluster_centers_\r\n",
        "# print(centroids.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-O77nhfQX80"
      },
      "source": [
        "#Source: https://stackoverflow.com/questions/26450673/sklearn-decomposition-top-terms\r\n",
        "# weights = np.dot(kmean.cluster_centers_, svd.components_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GURqR7uScFE"
      },
      "source": [
        "# weights.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHFgHb6fgmGT"
      },
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXSMX49YTmn7"
      },
      "source": [
        "# #this functon calculates cos theta between two vectors\r\n",
        "def similarity (vector_a, vector_b):\r\n",
        "  ma = np.linalg.norm(vector_a)\r\n",
        "  mb = np.linalg.norm(vector_b)\r\n",
        "  sim = (np.matmul(vector_a,vector_b))/(ma * mb)\r\n",
        "  return sim\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kVLd61UUsvL"
      },
      "source": [
        "num_cluster = kmean.n_clusters\r\n",
        "centroid_matrix = np.zeros((num_cluster,num_cluster))\r\n",
        "centroids = kmean.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcV1a6O4UOny"
      },
      "source": [
        "\r\n",
        "for i in range(num_cluster):\r\n",
        "  for j in range(num_cluster):\r\n",
        "    centroid_matrix[i][j] = similarity(centroids[i], centroids[j])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cngirXbMVPby"
      },
      "source": [
        "# print(centroid_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGqyG92nVd7l"
      },
      "source": [
        "#Source: https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\r\n",
        "# df_cm = pd.DataFrame(centroid_matrix, range(1,num_cluster+1), range(1,num_cluster+1))\r\n",
        "# plt.figure(figsize = (200,140))\r\n",
        "# # sn.color_palette(\"vlag\", as_cmap=True)\r\n",
        "# plot = sn.heatmap(df_cm, annot=True, cmap=\"coolwarm\")\r\n",
        "# plot.figure.savefig(\"/content/drive/MyDrive/Final/output_250.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzowErA8Ndpw"
      },
      "source": [
        "# # # true_k=3\r\n",
        "# # #Source: https://pythonprogramminglanguage.com/kmeans-text-clustering/\r\n",
        "# text = \"\"\r\n",
        "# print(\"Top terms per cluster:\")\r\n",
        "# order_centroids = kmean.cluster_centers_.argsort()[:, ::-1]\r\n",
        "# terms = vectorizer.get_feature_names()\r\n",
        "\r\n",
        "# for i in range(kmean.n_clusters):\r\n",
        "#   print(\"Cluster %d:\" % i),\r\n",
        "#   for ind in order_centroids[i, :1000]:\r\n",
        "\r\n",
        "#     # print(len(terms))\r\n",
        "#     text +=  terms[ind] + \" \"\r\n",
        "\r\n",
        "#       # print(' %s' % terms[ind])\r\n",
        "#   text =  text.replace(\"gt\", \"\")\r\n",
        "#   print(text)\r\n",
        "#   draw_cloudWord(text, \"cloud_tfidf_\"+str(i))\r\n",
        "#   text = \"\"\r\n",
        "#   print"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7TfZvJxNe9J"
      },
      "source": [
        "##word cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeI12EG9Ndek"
      },
      "source": [
        "#source: https://www.datacamp.com/community/tutorials/wordcloud-python\r\n",
        "#https://pypi.org/project/wordcloud-fa/\r\n",
        "def draw_cloudWord(text, fileName):\r\n",
        "\r\n",
        "  wordcloud = WordCloudFa(no_reshape=False,  include_numbers=False, collocations=False, width=800, height=400,background_color=\"white\")\r\n",
        "  wc = wordcloud.generate(text)\r\n",
        "  # frequencies = wordcloud.process_text(text)\r\n",
        "  # wc = wordcloud.generate_from_frequencies(frequencies)\r\n",
        "  image = wc.to_image()\r\n",
        "  image.show()\r\n",
        "  image.save('/content/drive/MyDrive/Final/cloudFolder250/'+fileName +\".png\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIWCG0cj3RKL"
      },
      "source": [
        "def writeToFile(word, weight,  fileName, lastFileName):\r\n",
        "    X_and_Y = pd.read_csv(fileName , encoding=\"utf-8\")\r\n",
        "    text = ''\r\n",
        "    counter = 0\r\n",
        "    for item in input_list:\r\n",
        "        for i in range(len(item)):\r\n",
        "            text += list(item)[i] + ' '\r\n",
        "        print(counter)    \r\n",
        "        X_and_Y.loc [counter, 'text'] = text.strip()\r\n",
        "        X_and_Y.to_csv(lastFileName ,index=False, encoding='utf-8-sig')\r\n",
        "        counter = counter + 1\r\n",
        "        text =''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9lkjuyn3RHh"
      },
      "source": [
        "# features = vectorizer.get_feature_names()\r\n",
        "# file = pd.read_csv(\"/content/drive/MyDrive/Final/TF_IDF_00.csv\" , encoding=\"utf-8\")\r\n",
        "# counter = 0\r\n",
        "# centroids = kmean.cluster_centers_\r\n",
        "# text = \"\"\r\n",
        "# weights = np.abs(centroids)\r\n",
        "# for i in range(kmean.n_clusters):\r\n",
        "#   print(\"Cluster %d:\" % i)\r\n",
        "#   file.loc [counter, 'Cluster'] = \"Cluster %d:\" % i\r\n",
        "#   top5 = np.argsort(weights[i])[-50:]\r\n",
        "#   for j in top5:\r\n",
        "#     text +=features[j].strip() +\" \"\r\n",
        "#     print(\"Word: \"+ features[j] + \" weight: \"+ str(weights[i][j] ))\r\n",
        "#     counter = counter + 1\r\n",
        "#     file.loc [counter, 'Word'] = features[j].strip()\r\n",
        "#     file.loc [counter, 'Weight'] = weights[i][j]\r\n",
        "    \r\n",
        "    \r\n",
        "#   text =  text.replace(\"gt\", \"\")\r\n",
        "#   print(text)\r\n",
        "#   draw_cloudWord(text, \"cloud_tfidf_\"+str(i))\r\n",
        "#   file.to_csv(\"/content/drive/MyDrive/Final/TF_IDF_00.csv\" ,index=False, encoding='utf-8-sig')\r\n",
        "#   text = \"\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "716rQOpfIGbb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoQpETLq9B_9"
      },
      "source": [
        "# features = vectorizer.get_feature_names()\r\n",
        "file = pd.read_csv(\"/content/drive/MyDrive/Final/tf-idf250.csv\" , encoding=\"utf-8\")\r\n",
        "file1 = pd.read_csv(\"/content/drive/MyDrive/Final/tweets_cluster_250.csv\" , encoding=\"utf-8\")\r\n",
        "counter = 0\r\n",
        "counter1 = 0\r\n",
        "content = \"\"\r\n",
        "#each row == each tweet\r\n",
        "distances = kmean.transform(X=X_svd)\r\n",
        "print(distances.shape)\r\n",
        "centroids = kmean.cluster_centers_\r\n",
        "text = \"\"\r\n",
        "# weights = np.abs(centroids)\r\n",
        "for i in range(kmean.n_clusters):\r\n",
        "  print(\"Cluster %d:\" % i)\r\n",
        "  file.loc [counter, 'Cluster'] = \"Cluster %d:\" % i\r\n",
        "  file1.loc [counter1, 'Cluster'] = \"Cluster %d:\" % i\r\n",
        "  top5 = np.argsort(distances[:, i])[0:49] \r\n",
        "\r\n",
        "  for j in top5:\r\n",
        "    print(j)\r\n",
        "    index = list_x_and_y[j]\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "    for word in range(len(index)):\r\n",
        "      \r\n",
        "      text +=index[word].strip() +\" \"\r\n",
        "      content  +=index[word].strip() +\" \"\r\n",
        " \r\n",
        "      print(\"Word: \"+ index[word] + \" distance: \"+ str(distances[j][i] ))\r\n",
        "      counter = counter + 1\r\n",
        "      file.loc [counter, 'Word'] = index[word]\r\n",
        "      file.loc [counter, 'Weight'] = distances[j][i]\r\n",
        "    \r\n",
        "    \r\n",
        "    text =  text.replace(\"gt\", \"\")\r\n",
        "    counter1 = counter1 +1\r\n",
        "    file1.loc [counter1, 'tweet'] = content\r\n",
        "    file1.to_csv(\"/content/drive/MyDrive/Final/tweets_cluster_250.csv\" ,index=False, encoding='utf-8-sig')\r\n",
        "    print(text)\r\n",
        "    content = \"\"\r\n",
        "  draw_cloudWord(text, \"cloud_tfidf_\"+str(i))\r\n",
        "  file.to_csv(\"/content/drive/MyDrive/Final/tf-idf250.csv\" ,index=False, encoding='utf-8-sig')\r\n",
        "  text = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw4jzvHZ3RFW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEbRyyHM3RDL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I13UadzzegtP"
      },
      "source": [
        "# y_kmeans = kmean.predict(X_train_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqWhJI_Zc4XH"
      },
      "source": [
        "#we can see our three centers by using the following command\r\n",
        "# kmean.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0Vlya2ZdCaQ"
      },
      "source": [
        "#To check the labels created, we can use the following command. It gives the labels created for our data\r\n",
        "# kmean.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnsmeXnEe0L5"
      },
      "source": [
        "#Let’s evaluate how well the formed clusters are. To do that, we will calculate the inertia of the clusters:\r\n",
        "# kmean.inertia_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_IqWce8e7Vy"
      },
      "source": [
        "# frame = pd.DataFrame(X_train_vector)\r\n",
        "# frame['cluster'] = y_kmeans\r\n",
        "# frame['cluster'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXj-pBsagjLY"
      },
      "source": [
        "# fitting multiple k-means algorithms and storing the values in an empty list\r\n",
        "#source: https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/\r\n",
        "#source:https://realpython.com/k-means-clustering-python/\r\n",
        "SSE = []\r\n",
        "SIL = []\r\n",
        "\r\n",
        "clusterList = [22, 70, 100,150,200,250,300,350,400,500,700,1000]\r\n",
        "for cluster in clusterList:\r\n",
        "    kmeans = KMeans(n_jobs = -1, n_clusters = cluster, init='k-means++')\r\n",
        "    kmeans.fit(X_svd,sample_weight=Y)\r\n",
        "    SSE.append(kmeans.inertia_)\r\n",
        "    labels = kmeans.labels_\r\n",
        "    SIL.append(metrics.silhouette_score(X_svd, labels, metric='euclidean'))\r\n",
        "    # clusterList.append(cluster)\r\n",
        "    print (cluster)\r\n",
        "    # cluster = cluster + 10\r\n",
        "\r\n",
        "# converting the results into a dataframe and plotting them\r\n",
        "# frame = pd.DataFrame({'Cluster':clusterList, 'SSE':SSE})\r\n",
        "# plt.figure(figsize=(12,6))\r\n",
        "# plt.plot(frame['Cluster'], frame['SSE'], marker='o')\r\n",
        "# plt.xlabel('Number of clusters')\r\n",
        "# plt.ylabel('silhouette_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J82jEHpoZ0l"
      },
      "source": [
        " \r\n",
        "# clusterList = [100,  500,1000,2000,3000,3500,4000,4500,5000]\r\n",
        "# frame = pd.DataFrame({'Cluster':clusterList, 'SSE':SSE})\r\n",
        "# plt.figure(figsize=(12,6))\r\n",
        "# plt.plot(frame['Cluster'], frame['SSE'], marker='o')\r\n",
        "# plt.xlabel('Number of clusters')\r\n",
        "# plt.ylabel('silhouette_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNxPe0fxGKuO"
      },
      "source": [
        "# print(len (SSE))\r\n",
        "frame = pd.DataFrame({'Cluster':clusterList, 'SIL':SIL})\r\n",
        "plt.figure(figsize=(20,10))\r\n",
        "plt.plot(frame['Cluster'], frame['SIL'], marker='o')\r\n",
        "plt.xlabel('Number of clusters')\r\n",
        "plt.ylabel('silhouette_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Din3IlIcnc9a"
      },
      "source": [
        "frame = pd.DataFrame({'Cluster':clusterList, 'SSE':SSE})\r\n",
        "plt.figure(figsize=(12,6))\r\n",
        "plt.plot(frame['Cluster'], frame['SSE'], marker='o')\r\n",
        "plt.xlabel('Number of clusters')\r\n",
        "plt.ylabel('Inertia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S9_siwtib-D"
      },
      "source": [
        "kl = KneeLocator(clusterList, SSE, curve=\"convex\", direction=\"decreasing\")\r\n",
        "true_k = kl.elbow\r\n",
        "print (true_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STT-REXZ7Pqq"
      },
      "source": [
        "true_k = 250\r\n",
        "kmean=KMeans(n_clusters=true_k)\r\n",
        "kmean.fit(X_svd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nf-Y7qLrEGt"
      },
      "source": [
        "labels = kmean.labels_\r\n",
        "print(metrics.silhouette_score(X_svd, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqmZOaE07qRK"
      },
      "source": [
        "# true_k=3\r\n",
        "#Source: https://pythonprogramminglanguage.com/kmeans-text-clustering/\r\n",
        "# print(\"Top terms per cluster:\")\r\n",
        "# order_centroids = kmean.cluster_centers_.argsort()[:, ::-1]\r\n",
        "# terms = vectorizer.get_feature_names()\r\n",
        "# for i in range(true_k):\r\n",
        "#     print(\"Cluster %d:\" % i),\r\n",
        "#     for ind in order_centroids[i, :50]:\r\n",
        "#         print(' %s' % terms[ind]),\r\n",
        "#     print"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1mZB_dSKguh"
      },
      "source": [
        "# labels=kmean.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKWsRUj58yt1"
      },
      "source": [
        "\r\n",
        "# silhouette_score = metrics.silhouette_score(X_train_vector, labels, metric='euclidean')\r\n",
        "# print (\"Silhouette_score: \",silhouette_score )\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAX4r-4zTKVR"
      },
      "source": [
        "#https://www.w3resource.com/python-exercises/list/python-data-type-list-exercise-30.php\r\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvbtpjhclp2k"
      },
      "source": [
        "my_list = kmean.predict(X=X_svd)\r\n",
        "ctr = collections.Counter(my_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMbk7PcYlpzV"
      },
      "source": [
        "dict(sorted(ctr.items(), key=lambda item: item[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6F3EueflpxE"
      },
      "source": [
        "print(ctr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frVyOBJNlpvI"
      },
      "source": [
        "for i in range(len(ctr)):\r\n",
        "  print (i ,ctr[i] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBh7pZC7TKvy"
      },
      "source": [
        "# Give weight to each tweet based on freqency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwNmZm3_TKSo"
      },
      "source": [
        "X_and_Y = removeEmptyRows ('/content/drive/MyDrive/Final/train_temp.csv')\r\n",
        "X = X_and_Y.text\r\n",
        "sorted_tweets_list = sorted(X)\r\n",
        "# dictionary = dict()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAJxgMxhTKQC"
      },
      "source": [
        "def word_count(sorted_tweets_list):\r\n",
        "  # file = pd.read_csv(\"/content/drive/MyDrive/Final/WightedTweets.csv\" , encoding=\"utf-8\")\r\n",
        "  # counter = 0 \r\n",
        "  dictionary = dict()\r\n",
        "  for text in sorted_tweets_list:\r\n",
        "    print(text)\r\n",
        "    tweet = text.strip()\r\n",
        "    key = tweet\r\n",
        "    if key in dictionary.keys():\r\n",
        "      dictionary[key] += 1\r\n",
        "      \r\n",
        "      # file.loc [counter, 'Weight'] = dictionary[key]\r\n",
        "  \r\n",
        "    else:\r\n",
        "      \r\n",
        "      dictionary[key] = 1\r\n",
        "      # file.loc [counter, 'tweet'] = tweet\r\n",
        "      # file.loc [counter, 'Weight'] = dictionary[key]\r\n",
        "      # counter = counter + 1 \r\n",
        "      \r\n",
        "  return dictionary\r\n",
        "    # file.to_csv(\"/content/drive/MyDrive/Final/WightedTweets.csv\" ,index=False, encoding='utf-8-sig')\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdTAmZsIqooA"
      },
      "source": [
        "dictionary = word_count(sorted_tweets_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfFc0n2qqoi2"
      },
      "source": [
        "file = pd.read_csv(\"/content/drive/MyDrive/Final/WightedTweets.csv\" , encoding=\"utf-8\")\r\n",
        "counter = 0 \r\n",
        "for key in dictionary.keys():\r\n",
        "  file.loc [counter, 'tweet'] = key\r\n",
        "  file.loc [counter, 'Weight'] = dictionary[key]\r\n",
        "  print(counter)\r\n",
        "  counter += 1\r\n",
        "  \r\n",
        "  file.to_csv(\"/content/drive/MyDrive/Final/WightedTweets.csv\" ,index=False, encoding='utf-8-sig')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGZ4_ZMAHK0u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}