{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SheidaTalei_BERT_FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SheidaTalei/FinalProject/blob/main/SheidaTalei_BERT_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRsGq3w763zd",
        "outputId": "7401a725-2c4d-45df-bc4f-bf2252f7a17a"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQDHHm9LH6f2",
        "outputId": "5f5e1ccc-df14-41b3-80e0-c41c8e8bde4b"
      },
      "source": [
        "!pip install --target=$nb_path transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Processing /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45/sacremoses-0.0.43-cp37-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK6F5D0cstnH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPTOzk5wtokN"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79CLtuSEtokc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import regex\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "from transformers import TFBertModel, TFBertPreTrainedModel, TFBertForSequenceClassification\n",
        "from transformers import glue_convert_examples_to_features, InputExample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8A7rreWwhux"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57rCgTgItokj"
      },
      "source": [
        "# %run SheidaTalei_Preprocessing_FinalProject.ipynb\n",
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWkO2lBGtolM"
      },
      "source": [
        "#------------------------------------------Empty Rows Removal----------------------------------------------------------------\n",
        "# This function Will: 1- Remove all empty rows from csv file 2- Save data to the same csv\n",
        "def removeEmptyRows(fileName):\n",
        "    df = pd.read_csv(fileName, encoding='utf-8-sig')\n",
        "    df = df.dropna(subset=['text'], how='all', axis=0) \n",
        "    df.to_csv(fileName, header=True, encoding='utf-8-sig',  index=False)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C5GA3F5z49k"
      },
      "source": [
        "# df1= removeEmptyRows ('/content/drive/MyDrive/Final/Prepared_Data_1_Labeled.csv')\r\n",
        "# df2 =  removeEmptyRows ('/content/drive/MyDrive/Final/Prepared_train_Data_test#1.csv')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgK6pENDtolV"
      },
      "source": [
        "# X_and_Y = pd.concat([df1, df2], ignore_index=True, names=['tweeter_handle', 'text', 'Label', 'statues_count' ])\n",
        "X_and_Y = removeEmptyRows ('/content/drive/MyDrive/Final/train_temp.csv')\n",
        "train_data, valid_data = train_test_split(X_and_Y, test_size=0.3)\n",
        "Y_train= train_data.Label\n",
        "X_train = train_data.text\n",
        "\n",
        "\n",
        "X_valid = valid_data.text\n",
        "Y_valid = valid_data.Label\n",
        "\n",
        "# data_frame= pd.concat([df1, df2, df3, df4, df5 ], ignore_index=True, names=['tweeter_handle', 'text', 'Label', 'statues_count' ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7zZ1KmaAd4e"
      },
      "source": [
        "count_0 = 0\r\n",
        "count_1 = 0\r\n",
        "\r\n",
        "for i in Y_valid:\r\n",
        "    if (i==0):\r\n",
        "        count_0 = count_0+1\r\n",
        "    else:\r\n",
        "        count_1 = count_1 + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AFtrTMGAlCj"
      },
      "source": [
        "print(\"count_0 \",count_0)\r\n",
        "print(\"count_1 \",count_1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJVF7FXDAsVE"
      },
      "source": [
        "count_0 = 0\r\n",
        "count_1 = 0\r\n",
        "\r\n",
        "for i in Y_train:\r\n",
        "    if (i==0):\r\n",
        "        count_0 = count_0+1\r\n",
        "    else:\r\n",
        "        count_1 = count_1 + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DChqkRf8AwIT"
      },
      "source": [
        "print(\"count_0 \",count_0)\r\n",
        "print(\"count_1 \",count_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GQlreTX7Fy1"
      },
      "source": [
        "# X_and_Y_test = removeEmptyRows('/content/drive/MyDrive/Final/Prepared_test_Data_test#1.csv')\n",
        "X_and_Y_test = removeEmptyRows('/content/drive/MyDrive/Final/test_temp.csv')\n",
        "Y_test= X_and_Y_test.Label\n",
        "X_test = X_and_Y_test.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gz_gI8WA3Ed"
      },
      "source": [
        "count_0 = 0\r\n",
        "count_1 = 0\r\n",
        "\r\n",
        "for i in Y_test:\r\n",
        "    if (i==0):\r\n",
        "        count_0 = count_0+1\r\n",
        "    else:\r\n",
        "        count_1 = count_1 + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZYPhsCoA5Ya",
        "outputId": "ca46ba69-0667-44f6-b401-9583ff34f439"
      },
      "source": [
        "print(\"count_0 \",count_0)\r\n",
        "print(\"count_1 \",count_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count_0  5200\n",
            "count_1  5254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGHWRBp74DO_"
      },
      "source": [
        "# X_and_Y_needLabel = removeEmptyRows('/content/drive/MyDrive/Final/Prepared_Data_1.csv')\r\n",
        "# Y_needLabel= X_and_Y_needLabel.Label\r\n",
        "# X_needLabel = X_and_Y_needLabel.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8WOo3WAtomP"
      },
      "source": [
        "#source:\n",
        "#https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/\n",
        "#https://skimai.com/fine-tuning-bert-for-sentiment-analysis/\n",
        "#https://sci2lab.github.io/ml_tutorial/bert_farsi_sentiment/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwO4VH2ltomW"
      },
      "source": [
        "#We need to transform our data into a format BERT understands. \n",
        "# data is a dataFrame\n",
        "def convert_data_into_input_example(data):\n",
        "    \"\"\" Covert the list of examples into a list of `InputExample` objects that is suitable\n",
        "        for BERT model.\"\"\"\n",
        "    input_examples = []\n",
        "    for i in range(len(data)):\n",
        "        example = InputExample(\n",
        "            guid= None,\n",
        "            text_a= data.iloc[i][ 'text'], #  is the text we want to classify\n",
        "            text_b= None,\n",
        "            label= data.iloc[i][ 'Label'] #is the label of our example, i.e. 1 or 0.\n",
        "        )\n",
        "        input_examples.append(example)\n",
        "    return input_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU_nJHamtomb"
      },
      "source": [
        "train_input_examples = convert_data_into_input_example(X_and_Y)\n",
        "val_input_examples = convert_data_into_input_example(valid_data)\n",
        "#\n",
        "# needLabel_input_example = convert_data_into_input_example(X_and_Y_needLabel)\n",
        "test_input_examples = convert_data_into_input_example(X_and_Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow1_nW7FJ5Z4",
        "outputId": "f0ec02b6-1ebd-40da-c8ef-7491e1d39132"
      },
      "source": [
        "train_input_examples[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InputExample(guid=None, text_a='یار طلبید#طلب مونس زیاد جان دولت حاجت ماست صحبت بس ان', text_b=None, label=0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCTWUUsXtomg"
      },
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
        "\n",
        "# text = 'I liked that book very much!'\n",
        "# tokenized_text = tokenizer.tokenize(text)\n",
        "# print(tokenized_text)\n",
        "# text_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "# print('text ids:', text_ids)\n",
        "# text_ids_with_special_tokens = tokenizer.build_inputs_with_special_tokens(text_ids)\n",
        "# print('text ids with special tokens: ', text_ids_with_special_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUmZMUzctoml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ac743c-bdbb-4a6b-90d8-4bf39d25a928"
      },
      "source": [
        "#Tokenization\n",
        "label_list = [0.0, 1.0]\n",
        "MAX_SEQ_LENGTH = 150\n",
        "bert_train_dataset = glue_convert_examples_to_features(examples=train_input_examples, tokenizer=tokenizer, max_length=MAX_SEQ_LENGTH, task='mrpc', label_list=label_list)\n",
        "bert_val_dataset = glue_convert_examples_to_features(examples=val_input_examples, tokenizer=tokenizer, max_length=MAX_SEQ_LENGTH, task='mrpc', label_list=label_list)\n",
        "\n",
        "#\n",
        "bert_test_dataset =  glue_convert_examples_to_features(examples=test_input_examples, tokenizer=tokenizer, max_length=MAX_SEQ_LENGTH, task='mrpc', label_list=label_list)\n",
        "# bert_needLabel_dataset =  glue_convert_examples_to_features(examples=needLabel_input_example, tokenizer=tokenizer, max_length=MAX_SEQ_LENGTH, task='mrpc', label_list=label_list)\n",
        "\n",
        "# for i in range(3):\n",
        "# #     print('Example: {}'.format(bert_train_dataset[i]))\n",
        "#     print('Example: {')\n",
        "#     print(' Input_ids: {}'.format(bert_train_dataset[i].input_ids))\n",
        "#     print(' attention_mask: {}'.format(bert_train_dataset[i].attention_mask))\n",
        "#     print(' token_type_ids: {}'.format(bert_train_dataset[i].token_type_ids))\n",
        "#     print(' label: {}'.format(bert_train_dataset[i].label))\n",
        "#     print('}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/processors/glue.py:175: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ErScLAHtomo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f72503a-66b8-4356-c988-abc9b48a4acc"
      },
      "source": [
        "# #for myself\n",
        "# ex = bert_train_dataset[0]\n",
        "# in_ids = ex.input_ids\n",
        "# decoded_sentence = tokenizer.decode(in_ids, skip_special_tokens=True)\n",
        "# print(decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "یار طلبید # طلب مونس زیاد جان دولت حاجت ماست صحبت بس ان\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phkBMaX5PgtV"
      },
      "source": [
        "# import os\n",
        "# checkpoint_path = \"/content/drive/MyDrive/Final/Bert.ckpt\"\n",
        "# model = createLstmModel()\n",
        "# model.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjOlCA5ytomv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6af86af-3125-493b-e6fe-c359ed501317"
      },
      "source": [
        "#Defining the Hyperparameters\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels = 2)\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# metric_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric_accuracy , tf.keras.metrics.AUC()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVph06rBKE9D"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "#, tf.keras.metrics.AUC()\n",
        "# model.add(tf.keras.layers.Dense(nb_classes, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[ metric_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq39MAhStomy"
      },
      "source": [
        "#training the model\n",
        "# model.fit(bert_train_dataset, validation_data=bert_val_dataset, epochs=3)\n",
        "def my_solution(bdset):\n",
        "    \"\"\" Create a list of input tensors required to be in the first argument of the\n",
        "        model call function for training. e.g. `model([input_ids, attention_mask, token_type_ids])`.\n",
        "    \"\"\"\n",
        "    input_ids, attention_mask, token_type_ids, label = [], [], [], []\n",
        "    for in_ex in bdset:\n",
        "        input_ids.append(in_ex.input_ids)\n",
        "        attention_mask.append(in_ex.attention_mask)\n",
        "        token_type_ids.append(in_ex.token_type_ids)\n",
        "        label.append(in_ex.label)\n",
        "\n",
        "    input_ids = np.vstack(input_ids)\n",
        "    attention_mask = np.vstack(attention_mask)\n",
        "    token_type_ids = np.vstack(token_type_ids)\n",
        "    label = np.vstack(label)\n",
        "    return ([input_ids, attention_mask, token_type_ids], label)\n",
        "#Create a list of input tensors required to be in the first argument of the model call function for training. e.g.\n",
        "# model([input_ids, attention_mask, token_type_ids])."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHj3aiAGtom2"
      },
      "source": [
        "# Convert a training example into the Bert compatible format.\n",
        "def example_to_features(input_ids, attention_masks, token_type_ids, y):\n",
        "    \"\"\" Convert a training example into the Bert compatible format.\"\"\"\n",
        "    return {\"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_masks,\n",
        "            \"token_type_ids\": token_type_ids},y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQBRW-5RKofM"
      },
      "source": [
        "#source: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_load.ipynb#scrollTo=IFPuhwntH8VH\n",
        "import os\n",
        "checkpoint_path = \"/content/drive/MyDrive/Final/Bert6.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKkWuQH3qcMX"
      },
      "source": [
        "x_train, y_train = my_solution(bert_train_dataset)\r\n",
        "x_val, y_val = my_solution(bert_val_dataset)\r\n",
        "\r\n",
        "x_test, y_test = my_solution(bert_test_dataset)\r\n",
        "\r\n",
        "#\r\n",
        "# x_neededLabel, y_neededLabel = my_solution (bert_needLabel_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4UoSSWSqTk-"
      },
      "source": [
        "#This part helps for handeling imbalanced data\r\n",
        "count_0 = 0\r\n",
        "count_1 = 0\r\n",
        "\r\n",
        "for i in y_train:\r\n",
        "    if (i==0):\r\n",
        "        count_0 = count_0+1\r\n",
        "    else:\r\n",
        "        count_1 = count_1 + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnSCOrx-qXOk"
      },
      "source": [
        "p = count_0 / (count_0 + count_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NI29Dx2ydTX",
        "outputId": "ab675045-ed54-494c-cfae-95cde0c2e5c3"
      },
      "source": [
        "print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4952647438656909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEji_qMw5eBf",
        "outputId": "e7106f4b-ac4d-4056-8d55-0aea254f946a"
      },
      "source": [
        "print('x_train shape: {}'.format(x_train[0].shape))\r\n",
        "print('x_val shape: {}'.format(x_val[0].shape))\r\n",
        "\r\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train[0], x_train[1], x_train[2], y_train)).map(example_to_features).shuffle(100).batch(32)\r\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices((x_val[0], x_val[1], x_val[2], y_val)).map(example_to_features).batch(64)\r\n",
        "print(type (train_ds))\r\n",
        "print('Format of model input examples: {} '.format(train_ds.take(1)))\r\n",
        "\r\n",
        "#\r\n",
        "test_ds =  tf.data.Dataset.from_tensor_slices((x_test[0], x_test[1], x_test[2], y_test)).map(example_to_features).batch(64)\r\n",
        "# needLabel_ds = tf.data.Dataset.from_tensor_slices((x_neededLabel[0], x_neededLabel[1], x_neededLabel[2], y_neededLabel)).map(example_to_features).batch(64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (41814, 150)\n",
            "x_val shape: (12545, 150)\n",
            "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
            "Format of model input examples: <TakeDataset shapes: ({input_ids: (None, 150), attention_mask: (None, 150), token_type_ids: (None, 150)}, (None, 1)), types: ({input_ids: tf.int64, attention_mask: tf.int64, token_type_ids: tf.int64}, tf.int64)> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glTGwYLZtom7",
        "outputId": "ccdd2840-2b6a-4846-e6c4-c9ab1a347ffc"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "EPOCHS = 3 # 1000\n",
        "\n",
        "# history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS , batch_size = 150)\n",
        "# model.compile(optimizer=optimizer, loss=loss, metrics=[ metric_accuracy])\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS,  batch_size = 150, callbacks=[cp_callback] ) #callbacks=[cp_callback], class_weight = {0: 1/p, 1: 1/(1-p)} ,"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "1307/1307 [==============================] - ETA: 0s - loss: 0.5490 - accuracy: 0.7068WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "1307/1307 [==============================] - 1394s 1s/step - loss: 0.5490 - accuracy: 0.7068 - val_loss: 0.3287 - val_accuracy: 0.8589\n",
            "Epoch 2/3\n",
            "1307/1307 [==============================] - 1391s 1s/step - loss: 0.3396 - accuracy: 0.8498 - val_loss: 0.1982 - val_accuracy: 0.9205\n",
            "Epoch 3/3\n",
            "1307/1307 [==============================] - 1390s 1s/step - loss: 0.2449 - accuracy: 0.8988 - val_loss: 0.1352 - val_accuracy: 0.9507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wX-zYBfQVzU"
      },
      "source": [
        "# train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bgSbYuIQcJm"
      },
      "source": [
        "# train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqs_kEIFQfrc"
      },
      "source": [
        "# from keras.preprocessing.sequence import pad_sequences\r\n",
        "# train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGGrV_uPKrhm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHOGijwJKred"
      },
      "source": [
        "\r\n",
        "# tokens_tensor = torch.tensor([train_tokens_ids])\r\n",
        "# Put the model in \"evaluation\" mode,meaning feed-forward operation.\r\n",
        "# model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEnQMIl6MCfA"
      },
      "source": [
        "# segments_ids = [1] * len(tokenized_text)\r\n",
        "# print (segments_ids)\r\n",
        "# tokens_tensor = torch.tensor([train_tokens_ids])\r\n",
        "# segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGUQ1QFaKrY8"
      },
      "source": [
        "# with torch.no_grad():\r\n",
        "#     outputs = model(tokens_tensor,segments_tensors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx_ys3YCKrV8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKtDxa3KCofs"
      },
      "source": [
        "# can use last hidden state as word embeddings\r\n",
        "#source:https://medium.com/analytics-vidhya/bert-word-embeddings-deep-dive-32f6214f02bf\r\n",
        "last_hidden_state = history[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw4b7IZFLE2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed94f8a7-b2de-48ad-cdea-17c13f7600f8"
      },
      "source": [
        "#Loading the model\n",
        "import os\n",
        "checkpoint_path = \"/content/drive/MyDrive/Final/Bert5.ckpt\"\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels = 2)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[ metric_accuracy])\n",
        "model.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3249afc4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jScunXX23HUI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7MGPdjA3HJM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkOU_hKnWMHq"
      },
      "source": [
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20e47JXrtom_"
      },
      "source": [
        "# model.fit(bert_train_dataset, validation_data=bert_val_dataset, epochs=3)\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iqzDKEhtonC"
      },
      "source": [
        "# predictions = model.predict(test_ds)\n",
        "# print(predictions[0].shape)\n",
        "# print()\n",
        "# predictions_classes = np.argmax(predictions[0], axis = 1)\n",
        "# for i in range(500):\n",
        "#     print('text: {}\\n, actual label: {}, predicted label: {}'.format(X_and_Y_test.iloc[i]['text'], val_input_examples[i].label, predictions_classes[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBphnH5-6QMo"
      },
      "source": [
        "# def writeLabelInCsv(input_list,  fileName, lastFileName):\r\n",
        "#   X_and_Y = pd.read_csv(fileName , encoding=\"utf-8\")\r\n",
        "#   counter = 34188\r\n",
        "#   updator = 34188\r\n",
        "#   for item in range(len(input_list)):\r\n",
        "#     if counter <=80000:\r\n",
        "#       X_and_Y.loc [counter, 'Label'] = input_list [item + updator] \r\n",
        "#       X_and_Y.to_csv(lastFileName ,index=False, encoding='utf-8-sig')\r\n",
        "#       print( counter)\r\n",
        "#       counter = counter + 1\r\n",
        "\r\n",
        "#   print(\"counter: \", counter)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGTaO6oZGC-v"
      },
      "source": [
        "\r\n",
        "#add label to datasets\r\n",
        "prediction = model.predict(needLabel_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWTkvW_ptwO4"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3Nl6MC6IUCO"
      },
      "source": [
        "# load additional module\r\n",
        "# import pickle\r\n",
        "\r\n",
        "# # define a list of places\r\n",
        "\r\n",
        "# # file_name = \"/content/drive/MyDrive/Final/label_list_#1.pkl\"\r\n",
        "# # open_file = open(file_name, \"wb\")\r\n",
        "# # pickle.dump(np.argmax(prediction[0], axis = 1), open_file)\r\n",
        "# # open_file.close()\r\n",
        "\r\n",
        "# file_name = \"/content/drive/MyDrive/Final/label_list_#1.pkl\"\r\n",
        "# open_file = open(file_name, \"rb\")\r\n",
        "# label_list = pickle.load(open_file)\r\n",
        "# open_file.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZkTdjV8IZbm"
      },
      "source": [
        "# writeLabelInCsv(label_list,  '/content/drive/MyDrive/Final/Prepared_Data_1.csv', '/content/drive/MyDrive/Final/Prepared_Data_1_Labeled.csv')\r\n",
        "# import pandas as pd\r\n",
        "# writeLabelInCsv(label_list,  '/content/drive/MyDrive/Final/Prepared_Data_1_Labeled.csv', '/content/drive/MyDrive/Final/Prepared_Data_1_Labeled.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LExG8dXu3Zzf",
        "outputId": "f0a2f43f-5f47-4c4b-ab87-ad251bc4d7d4"
      },
      "source": [
        "#newly added\r\n",
        "predictions = model.predict(test_ds)\r\n",
        "predictions_classes = np.argmax(predictions[0], axis = 1)\r\n",
        "# print(predictions_classes)\r\n",
        "print ('Accuracy for BERT: ',accuracy_score(y_test, predictions_classes))\r\n",
        "print('F1-score for BERT: ', f1_score(y_test, predictions_classes))\r\n",
        "print('roc_auc_score for BERT: ', roc_auc_score(y_test, predictions_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f32e19ede50>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f32e19ede50>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f32fce16c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f32fce16c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Accuracy for BERT:  0.8610101396594605\n",
            "F1-score for BERT:  0.8583682620138415\n",
            "roc_auc_score for BERT:  0.8611294691224268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcw7tEhstonF"
      },
      "source": [
        "#Convert the test examples into Bert compatible format.\n",
        "def example_to_features_predict(input_ids, attention_masks, token_type_ids):\n",
        "    \"\"\"\n",
        "        Convert the test examples into Bert compatible format.\n",
        "    \"\"\"\n",
        "    return {\"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_masks,\n",
        "            \"token_type_ids\": token_type_ids}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyTNA2bBtonI"
      },
      "source": [
        "\n",
        "def get_prediction(in_sentences):\n",
        "    \"\"\"\n",
        "        Prepare the test comments and return the predictions.\n",
        "    \"\"\"\n",
        "    label_list = [0.0, 1.0]\n",
        "    input_examples = [InputExample(guid=\"\", text_a = x, text_b = None, label = 0.0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "    predict_input_fn = glue_convert_examples_to_features(examples=input_examples, tokenizer=tokenizer, max_length=MAX_SEQ_LENGTH, task='mrpc', label_list=label_list)\n",
        "    x_test_input, y_test_input = my_solution(predict_input_fn)\n",
        "    test_ds   = tf.data.Dataset.from_tensor_slices((x_test_input[0], x_test_input[1], x_test_input[2])).map(example_to_features_predict).batch(32)\n",
        "\n",
        "    predictions = model.predict(test_ds)\n",
        "    #   print('predictions:', predictions[0].shape)\n",
        "    predictions_classes = np.argmax(predictions[0], axis = 1)\n",
        "    return [(sentence, prediction) for sentence, prediction in zip(in_sentences, predictions_classes)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r6N1yzItonM"
      },
      "source": [
        "#Prepare the test comments and return the predictions\n",
        "\n",
        "pred_sentences = []\n",
        "i = 0\n",
        "while(i<20):\n",
        "  for tweet in X_and_Y_test.text:\n",
        "    pred_sentences.append(tweet)\n",
        "    i +=1\n",
        "predictions = get_prediction(pred_sentences)\n",
        "for p in predictions:\n",
        "    print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ90MBCjZiSr"
      },
      "source": [
        "print(\"Evaluating the BERT model\")\n",
        "model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_W1PKK_cII4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}