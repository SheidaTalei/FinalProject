{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SheidaTalei_KMeanVsBERT_FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNq31MKLm6Qob/gKR/KCUYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SheidaTalei/FinalProject/blob/main/SheidaTalei_KMeanVsBERT_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_ryjPNL0ZV1"
      },
      "source": [
        "#SUBJECT: K-Mean Vs BERT Embedding\r\n",
        "\r\n",
        "###AUTHOR: Sheida Talei"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knOOWv-A0mHT"
      },
      "source": [
        "import os, sys\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "nb_path = '/content/notebooks'\r\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\r\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEzzIl3mEOwk"
      },
      "source": [
        "import transformers\r\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "from pylab import rcParams\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib import rc\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report\r\n",
        "from collections import defaultdict\r\n",
        "from textwrap import wrap\r\n",
        "from torch import nn, optim\r\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBqHxMDVEcjX"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "#Convert a collection of raw documents to a matrix of TF-IDF features.\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "import regex\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "import pickle\r\n",
        "import re\r\n",
        "import string\r\n",
        "from transformers import TFBertModel, TFBertPreTrainedModel, TFBertForSequenceClassification\r\n",
        "from transformers import glue_convert_examples_to_features, InputExample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3WznH3oEPpc"
      },
      "source": [
        "#Loading the model\r\n",
        "import os\r\n",
        "checkpoint_path = \"/content/drive/MyDrive/Final/Bert1.ckpt\"\r\n",
        "\r\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels = 2, output_hidden_states = True)\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "metric_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\n",
        "\r\n",
        "\r\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[ metric_accuracy])\r\n",
        "model.load_weights(checkpoint_path)\r\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHL8QCY8Ef_q"
      },
      "source": [
        "# can use last hidden state as word embeddings\r\n",
        "#source:https://medium.com/analytics-vidhya/bert-word-embeddings-deep-dive-32f6214f02bf\r\n",
        "with torch.no_grad():\r\n",
        "outputs = model(tokens_tensor, segments_tensors)\r\n",
        "\r\n",
        "\r\n",
        "last_hidden_state = output[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}